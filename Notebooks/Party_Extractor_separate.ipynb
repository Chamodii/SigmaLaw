{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Party_Extractor_separate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ4nfXBxXwd7",
        "outputId": "42c6ee25-e315-4ddd-c730-34688011f0fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7EZmmJTSOkj"
      },
      "source": [
        "from random import randint\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, Dense, Maximum, LayerNormalization\n",
        "from keras.utils import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf\n",
        "import time, random\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_aPnjR5MgJv"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "data_path = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"Data_W2V\"\n",
        "Data_path = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"classifier\" / \"data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtze42TYFOA",
        "outputId": "f0386cf3-aa26-4880-b8df-997cadddcd5b"
      },
      "source": [
        "!pip install stanfordcorenlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordcorenlp\n",
            "  Downloading https://files.pythonhosted.org/packages/35/cb/0a271890bbe3a77fc1aca2bc3a58b14e11799ea77cb5f7d6fb0a8b4c46fa/stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from stanfordcorenlp) (5.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordcorenlp) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordcorenlp) (2020.12.5)\n",
            "Installing collected packages: stanfordcorenlp\n",
            "Successfully installed stanfordcorenlp-3.9.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCsq3NxOcqv-"
      },
      "source": [
        "import json\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "\n",
        "nlp = StanfordCoreNLP(r'/content/drive/Shared drives/SigmaLaw/stanford-corenlp-full-2018-10-05', quiet=False)\n",
        "props = {'annotators': 'tokenize, ner, coref', 'pipelineLanguage': 'en'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5YNaubPxvUD"
      },
      "source": [
        "DATA_PATH = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(DATA_PATH / 'GoogleNews-vectors-negative300.bin', binary=True, unicode_errors='ignore')\n",
        "from gensim.models import Word2Vec "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j110IaUdFUA"
      },
      "source": [
        "def build_complete_ner(word, ner, l):\n",
        "  if (len(l)>0):\n",
        "    if (l[0].get('ner') == ner):\n",
        "      word = word + \" \" + build_complete_ner(l[0].get(\"word\"), l[0].get('ner'), l[1:len(l)])\n",
        "    #print(\"built\" + word)\n",
        "    return word\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeFizBmPdPyC"
      },
      "source": [
        "def remove_non_ascii_1(text):\n",
        "  return ''.join(i for i in text if ord(i)<128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpjzPdgC0RHn"
      },
      "source": [
        "def getMaskValues(ner_dict):\n",
        "  masks_dict = {}\n",
        "  keys = list(ner_dict.keys())\n",
        "  for i in range(len(keys)):\n",
        "    if ner_dict[keys[i]] == \"P\":\n",
        "      value = 0.5*((1+i)/len(keys))\n",
        "    elif ner_dict[keys[i]] == \"O\":\n",
        "      value = 0.5 + 0.25*((1+i)/len(keys))\n",
        "    elif ner_dict[keys[i]] == \"L\":\n",
        "      value = 0.75 + 0.25*((1+i)/len(keys))\n",
        "    masks_dict[keys[i]] = value\n",
        "  return masks_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt6o334S0UOH"
      },
      "source": [
        "def createVectorListFromToken(token_list, ner_list, masks_dict_list):\n",
        "\n",
        "  max_length = 0\n",
        "  vector_list = []\n",
        "  for i in range(0,len(token_list)):\n",
        "    sentence_vector = []\n",
        "    current_token_list = token_list[i]\n",
        "    current_ner_list = ner_list[i]\n",
        "    mask_dict = masks_dict_list[i]\n",
        "\n",
        "    assert len(current_token_list) == len(current_ner_list)\n",
        "\n",
        "    for j in range(0, len(current_token_list)):\n",
        "    \n",
        "      if (current_ner_list[j] == \"None\"):\n",
        "        try:\n",
        "          vec = np.append(model[current_token_list[j]], 0)\n",
        "         \n",
        "        except KeyError:\n",
        "          vec = np.zeros(301)\n",
        "        \n",
        "      else:\n",
        "        try:\n",
        "          vec = np.append(np.zeros(300), mask_dict[current_token_list[j]])\n",
        "        except KeyError:    \n",
        "          vec = np.zeros(301)        \n",
        "        \n",
        "      sentence_vector.append(vec)\n",
        "\n",
        "    vector_list.append(sentence_vector)\n",
        "\n",
        "\n",
        "\n",
        "  return vector_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNF7GzOoyvsF"
      },
      "source": [
        "def makeTokenNERListsFromParagraph(text):\n",
        "  result = json.loads(nlp.annotate(text, properties=props))\n",
        "  sentences = result['sentences']\n",
        "  corefs = result['corefs']\n",
        "\n",
        "  sentence_tokens_test = []\n",
        "  sentence_ners_test = []\n",
        "  ner_test = {}\n",
        "  for each in sentences:\n",
        "        \n",
        "        q = each.get('tokens')\n",
        "\n",
        "        for j in range(0,len(q)):\n",
        "          sentence_tokens_test.append(q[j].get('word'))\n",
        "\n",
        "          if (q[j].get(\"ner\") == \"PERSON\" or q[j].get(\"ner\") == \"ORGANIZATION\" or q[j].get(\"ner\") == \"LOCATION\"):\n",
        "            word = build_complete_ner(q[j].get(\"word\"), q[j].get(\"ner\"), q[j+1 : len(q)])\n",
        "            if (q[j].get(\"ner\") == \"PERSON\"):\n",
        "              prefix = \"P\"\n",
        "            elif (q[j].get(\"ner\") == \"ORGANIZATION\") :\n",
        "              prefix = \"O\"\n",
        "            else:\n",
        "              prefix = \"L\"\n",
        "            if (j==0):\n",
        "              ner_test[word] = prefix\n",
        "              words = word.split(\" \")\n",
        "              for l in words:\n",
        "                sentence_ners_test.append(prefix)\n",
        "                \n",
        "            if (j!=0 and q[j-1].get(\"ner\") != q[j].get(\"ner\")):\n",
        "              ner_test[word] = prefix\n",
        "              words = word.split(\" \")\n",
        "              for l in words:\n",
        "                sentence_ners_test.append(prefix)\n",
        "              \n",
        "          else:\n",
        "            sentence_ners_test.append(\"None\")\n",
        "\n",
        "  headwords_list = []\n",
        "  for sentence in sentences:\n",
        "    headwords = []\n",
        "    for each in sentence['tokens']:\n",
        "      if(each.get(\"ner\") == \"PERSON\" or each.get(\"ner\") == \"ORGANIZATION\" or each.get(\"ner\") == \"LOCATION\"):\n",
        "        headwords.append(each.get('word'))\n",
        "      else:\n",
        "        headwords.append(0)\n",
        "\n",
        "    headwords_list.append(headwords)\n",
        "\n",
        "  final_ner_dict = {}\n",
        "\n",
        "\n",
        "  cluster_list = []\n",
        "  for i in corefs.values():\n",
        "    party_value = False\n",
        "    for each in ner_test.keys():\n",
        "      if each in i[0]['text']:\n",
        "        party_value = True\n",
        "        final_ner_dict[i[0]['text']] = ner_test[each]\n",
        "    if party_value == True:\n",
        "      cluster = []\n",
        "      for j in i:\n",
        "        # print(j)\n",
        "        for index in range(j['startIndex']-1,j['endIndex']-1):\n",
        "          # print(j['position'][0], index)\n",
        "          headwords_list[j['position'][0]-1][index] = i[0]['text']\n",
        "        cluster.append(j['text'])\n",
        "\n",
        "      cluster_list.append(cluster)\n",
        "\n",
        "  final_headwords = []\n",
        "  for i in headwords_list:\n",
        "    for j in i:\n",
        "      final_headwords.append(j)\n",
        "\n",
        "  for ner in ner_test.keys():\n",
        "    val = True\n",
        "    for cluster_ner in final_ner_dict.keys():\n",
        "      if ner in cluster_ner:\n",
        "        val = False\n",
        "    if (val):\n",
        "      final_ner_dict[ner] = ner_test[ner]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "  return sentence_ners_test, final_headwords, final_ner_dict, sentence_tokens_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V0cQEeUgDHP"
      },
      "source": [
        "def Legal_entity_identifier(text):\n",
        "\n",
        "  new_text = remove_non_ascii_1(text)\n",
        "\n",
        "  ner_list, headwordsList, ner_dict, token_list = makeTokenNERListsFromParagraph(new_text)\n",
        "  mask_dict = getMaskValues(ner_dict)\n",
        "\n",
        "  vector_list = createVectorListFromToken([token_list],[ner_list],[mask_dict])\n",
        "\n",
        "  d = {x: 0 for x in ner_dict}\n",
        "\n",
        "  return new_text, d, headwordsList, vector_list\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjlhQJlCdWcY",
        "outputId": "f11a26e6-695b-4a10-8e40-342c3d393cb7"
      },
      "source": [
        "f = open(\"/content/drive/Shared drives/SigmaLaw/data_2/54.txt\", \"rb\")\n",
        "text = f.read()\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Donaghy began his career as an NBA referee in September 1994 and continued in that position for thirteen seasons.  Battista agreed to pay Donaghy a fee for each game in which Donaghy correctly picked the winner. \\xe2\\x80\\x82 Donaghy provided the picks to Martino, Martino relayed the information to Battista, and Battista placed the bets. \\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjiT5basXAMP"
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MRuUHtQjmwu"
      },
      "source": [
        "def Probability_Cal(legal_entities,headwords_list,plaintif_prob,defendant_prob):\n",
        "  plaintifs=legal_entities.copy()\n",
        "  defendants=legal_entities.copy()\n",
        "  count=legal_entities.copy()\n",
        "  plaintifs_list=[]\n",
        "  defendants_list=[]\n",
        "  for i in range(0,len(headwords_list)):\n",
        "    try: \n",
        "      plaintifs[headwords_list[i]]+=plaintif_prob[i]\n",
        "      defendants[headwords_list[i]]+=defendant_prob[i]\n",
        "      count[headwords_list[i]]+=1 \n",
        "    except KeyError:  \n",
        "        continue\n",
        "    else:\n",
        "        \n",
        "       continue\n",
        "  if (len(legal_entities)>0):\n",
        "    le_list=list(legal_entities)\n",
        "    for j in range(len(legal_entities)):\n",
        "        try:\n",
        "            p=plaintifs[le_list[j]]/count[le_list[j]]\n",
        "            d=defendants[le_list[j]]/count[le_list[j]]\n",
        "        except ZeroDivisionError:\n",
        "            p=0\n",
        "            d=0\n",
        "        print(le_list[j],\" :\",p,\" , \",d)\n",
        "        if(p>=0.5 and d<0.5):\n",
        "            plaintifs_list.append(le_list[j])\n",
        "        elif(d>=0.5 and p<0.5):\n",
        "            defendants_list.append(le_list[j])\n",
        "        elif(d<0.5 and p<0.5):\n",
        "            continue\n",
        "        elif(p>d):\n",
        "            plaintifs_list.append(le_list[j])\n",
        "        elif(d>p):\n",
        "            defendants_list.append(le_list[j])\n",
        "\n",
        "\n",
        "\n",
        "    return(plaintifs_list,defendants_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OzUwerhnccN"
      },
      "source": [
        "model1= load_model(Data_path/\"GRU_512\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppsyTnKLzUZk"
      },
      "source": [
        "def findLegalParties():\n",
        "    text=str(input(\"Enter the Legal Opinion Here: \"))\n",
        "    print(\"\\n\")\n",
        "    new_text, legal_entities, headwords_list, vector_list = Legal_entity_identifier(text)\n",
        "    max_length = 443\n",
        "    new_list = []\n",
        "    for i in vector_list: #padding the vectors\n",
        "        for j in range(len(i),max_length):\n",
        "\n",
        "            i.append(np.zeros(301))\n",
        "        new_list.append(i)\n",
        "    vector_array = np.array(new_list)\n",
        "    k=max_length-len(headwords_list) #padding the tokens\n",
        "    for n in range(k):\n",
        "        headwords_list.append('0')\n",
        "    dec=model1.predict(vector_array)\n",
        "    dec_p=dec[:,:,0]\n",
        "    dec_d=dec[:,:,1]\n",
        "    probabilities_p = dec_p.reshape(max_length)\n",
        "    probabilities_d = dec_d.reshape(max_length)\n",
        "    bin_list_p = probabilities_p.tolist()\n",
        "    bin_list_d = probabilities_d.tolist()\n",
        "    print(\"Legal Entity : Petitioner Probability , Defendant Probability\")\n",
        "    p,d=Probability_Cal(legal_entities,headwords_list,bin_list_p,bin_list_d)\n",
        "    print(\"\\n\")\n",
        "    print(\"Petitioners:\")\n",
        "    for i in range(len(p)):\n",
        "        print(p[i])\n",
        "    print(\"\\n\") \n",
        "    print(\"Defendants:\")\n",
        "    for i in range(len(d)):\n",
        "        print(d[i]) \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mJ2O_quz42_",
        "outputId": "ddaf601f-c8d3-419f-b95e-643d64e4d39a"
      },
      "source": [
        "findLegalParties()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Legal Opinion Here: Ranjan Ramanayake was sentenced to 4 years of rigorous imprisonment on the basis of contempt of court regarding his comments on the corruption of the judiciary.\n",
            "\n",
            "\n",
            "Legal Entity : Petitioner Probability , Defendant Probability\n",
            "Ranjan Ramanayake  : 0.3350548499359623  ,  0.0005733267956694968\n",
            "\n",
            "\n",
            "Petitioners:\n",
            "\n",
            "\n",
            "Defendants:\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}